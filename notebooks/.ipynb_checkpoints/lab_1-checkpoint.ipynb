{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef27949",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.expanduser('~/Research/lab/KAMemory/src'))\n",
    "import utils\n",
    "import torch\n",
    "from models import Autoencoder, MTL, logger\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10c547",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" settings \"\"\"\n",
    "\n",
    "# architecture sizes\n",
    "dim_ei = 50\n",
    "dim_ca3 = 50\n",
    "dim_ca1 = 50\n",
    "dim_eo = dim_ei\n",
    "\n",
    "# data settings\n",
    "nb_samples = 200\n",
    "num_reconstructions = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6565a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data generation \"\"\"\n",
    "\n",
    "# --- DISTRIBUTION 1 ---\n",
    "heads = 3\n",
    "variance = 0.05\n",
    "higher_heads = heads\n",
    "higher_variance = 0.075\n",
    "\n",
    "# make samples\n",
    "distrib_1 = utils.stimulus_generator(N=nb_samples, size=dim_ei,\n",
    "                             heads=heads, variance=variance,\n",
    "                             higher_heads=higher_heads,\n",
    "                             higher_variance=higher_variance,\n",
    "                             plot=False)\n",
    "test_distrib_1 = utils.stimulus_generator(N=nb_samples, size=dim_ei,\n",
    "                             heads=heads, variance=variance,\n",
    "                             higher_heads=higher_heads,\n",
    "                             higher_variance=higher_variance,\n",
    "                             plot=False)\n",
    "\n",
    "# --- DISTRIBUTION 2 ---\n",
    "heads = 2\n",
    "variance = 0.05\n",
    "higher_heads = heads \n",
    "higher_variance = 0.075\n",
    "\n",
    "# make samples\n",
    "distrib_2 = utils.stimulus_generator(N=nb_samples, size=dim_ei,\n",
    "                             heads=heads, variance=variance,\n",
    "                             higher_heads=higher_heads,\n",
    "                             higher_variance=higher_variance,\n",
    "                             plot=False)\n",
    "test_distrib_2 = utils.stimulus_generator(N=nb_samples, size=dim_ei,\n",
    "                             heads=heads, variance=variance,\n",
    "                             higher_heads=higher_heads,\n",
    "                             higher_variance=higher_variance,\n",
    "                             plot=False)\n",
    "\n",
    "# --- DISTRIBUTION 3 ---\n",
    "distrib_13 = utils.sparse_stimulus_generator(N=nb_samples,\n",
    "                                             size=dim_ei,\n",
    "                                             K = 5,\n",
    "                                             plot=True)\n",
    "\n",
    "# ----------------------\n",
    "# make one data dataset\n",
    "if bool(0):\n",
    "    training_samples = np.concatenate((distrib_2, distrib_1), axis=0)\n",
    "    test_samples = np.concatenate((test_distrib_2, test_distrib_1), axis=0)\n",
    "\n",
    "    # shuffle\n",
    "    training_samples = training_samples[torch.randperm(training_samples.shape[0])]\n",
    "    test_samples = test_samples[torch.randperm(test_samples.shape[0])]\n",
    "    logger.debug(\"using both distributions\")\n",
    "\n",
    "else:\n",
    "    training_samples = distrib_2\n",
    "    test_samples = test_distrib_2\n",
    "\n",
    "# dataset for btsp\n",
    "num_btsp_samples = 1\n",
    "num_reconstructions = 1\n",
    "training_sample_btsp = training_samples[np.random.choice(\n",
    "                        range(training_samples.shape[0]),\n",
    "                        num_btsp_samples, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- sparse data\n",
    "\n",
    "N = nb_samples\n",
    "K = 5\n",
    "training_samples = utils.sparse_stimulus_generator(N=N, K=K, size=dim_ei,\n",
    "                                                   plot=False)\n",
    "test_samples = utils.sparse_stimulus_generator(N=N, K=K, size=dim_ei,\n",
    "                                         plot=False)\n",
    "\n",
    "logger(\"<<< Data generated >>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db9227",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" AUTOENCODER training \"\"\"\n",
    "\n",
    "autoencoder = Autoencoder(input_dim=dim_ei,\n",
    "                          encoding_dim=dim_ca1)\n",
    "logger(f\"%Autoencoder: {autoencoder}\")\n",
    "\n",
    "# train autoencoder\n",
    "epochs = 100\n",
    "loss_ae, autoencoder = utils.train_autoencoder(\n",
    "                training_data=training_samples,\n",
    "                test_data=test_samples,\n",
    "                model=autoencoder,\n",
    "                epochs=int(epochs),\n",
    "                batch_size=5, learning_rate=1e-3)\n",
    "logger(f\"<<< Autoencoder trained [loss={loss_ae:.4f}] >>>\")\n",
    "\n",
    "# reconstruct data\n",
    "out_ae, latent_ae = utils.reconstruct_data(data=training_sample_btsp,\n",
    "                                num=num_btsp_samples,\n",
    "                                model=autoencoder,\n",
    "                                show=False, \n",
    "                                plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0d1eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "-------------------------\n",
    "CLOSE-UP ON MTL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe838c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL DEFINITION\n",
    "\n",
    "Kis = 10\n",
    "\n",
    "# imported weights\n",
    "w_ei_ca1, w_ca1_eo = autoencoder.get_weights()\n",
    "\n",
    "\n",
    "# input stimulus\n",
    "x_ei = torch.tensor(training_sample_btsp[4].reshape(-1, 1).astype(np.float32))\n",
    "\n",
    "out_ae = autoencoder(x_ei.reshape(1, -1)).detach().numpy().reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# random weights\n",
    "w_ei_ca3 = nn.Parameter(torch.randn(dim_ca3, dim_ei) / dim_ca3**0.7)\n",
    "w_ca3_ca1 = nn.Parameter(torch.zeros(dim_ca1, dim_ca3))\n",
    "\n",
    "# --- MODEL TRAINING\n",
    "\n",
    "# Forward pass through EI to CA3 to CA1\n",
    "x_ca3 = w_ei_ca3 @ x_ei\n",
    "IS = w_ei_ca1 @ x_ei\n",
    "\n",
    "# --- modify the instructive signal\n",
    "is_ = IS.detach().numpy().flatten().copy()\n",
    "clipped = np.where(np.abs(is_) > 0.1,\n",
    "             is_, 0).reshape(-1, 1).astype(np.float32)\n",
    "             # 1, 0).reshape(-1, 1).astype(np.float32)\n",
    "IS = torch.tensor(clipped)\n",
    "\n",
    "# ----- rule 1\n",
    "# betas = torch.zeros_like(IS)\n",
    "# betas[torch.topk(IS.flatten(), Kis).indices] = 1.\n",
    "\n",
    "# # betas = betas.reshape(IS.shape)\n",
    "# tiled_ca3 = x_ca3.flatten().repeat(dim_ca1, 1)\n",
    "# w_ca3_ca1 = nn.Parameter((1 - betas) * w_ca3_ca1 + betas * tiled_ca3)\n",
    "\n",
    "# ----- rule 2\n",
    "w_ca3_ca1_prime  = nn.Parameter(IS @ x_ca3.T)\n",
    "w_ca3_ca1 = nn.Parameter(w_ca3_ca1_prime)\n",
    "\n",
    "# -- TEST\n",
    "x_eo = w_ca1_eo @ (w_ca3_ca1 @ (w_ei_ca3 @ x_ei))\n",
    "\n",
    "out = x_eo.detach().numpy().reshape(1, -1)\n",
    "\n",
    "\n",
    "# --- PLOT\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 5),\n",
    "                               sharex=True)\n",
    "\n",
    "#\n",
    "ax1.imshow(x_ei.numpy().reshape(1, -1), aspect=\"auto\",\n",
    "           cmap=\"Greys\", vmin=0, vmax=1)\n",
    "ax1.set_ylabel(\"Input data\")\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax2.imshow(out_ae, aspect=\"auto\", cmap=\"Greys\",\n",
    "           vmin=0, vmax=1)\n",
    "ax2.set_ylabel(\"Autoencoder\")\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "ax3.imshow(out, aspect=\"auto\", cmap=\"Greys\",\n",
    "           vmin=0, vmax=1)\n",
    "ax3.set_ylabel(\"MTL\")\n",
    "ax3.set_xticks([])\n",
    "ax3.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ec2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(is_.flatten(), bins=10, alpha=0.4)\n",
    "plt.hist(clipped.flatten(), bins=10, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(IS.flatten(), label=\"$IS$\", alpha=0.4, density=True)\n",
    "plt.hist(x_ca3.detach().flatten(), label=\"$x_{CA3}$\", alpha=0.4, density=True)\n",
    "plt.legend()\n",
    "plt.title(\"Contributions for $W_{CA3, CA1}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4aec8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# x_ca3\n",
    "plt.hist(x_ca3.detach().flatten(), bins=10, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f57103",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(221)\n",
    "# plt.axis(\"off\")\n",
    "plt.imshow(w_ei_ca1.detach().numpy(), aspect=\"auto\", cmap=\"RdYlGn\",\n",
    "           vmin=-1., vmax=1.)\n",
    "plt.title(\"$W_{EI, CA1}$\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(x_ca3.detach().numpy().T, aspect=\"auto\",\n",
    "           cmap=\"RdYlGn\", vmin=-1., vmax=1.)\n",
    "plt.yticks([])\n",
    "plt.title(\"$x_{CA3}$\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(IS, aspect=\"auto\",\n",
    "           cmap=\"RdYlGn\", vmin=-1., vmax=1.)\n",
    "plt.xticks([])\n",
    "plt.title(\"$IS$\")\n",
    "\n",
    "plt.subplot(224)\n",
    "# plt.imshow(weica1, aspect=\"auto\", cmap=\"RdYlGn\",\n",
    "#            vmin=-1., vmax=1.)\n",
    "plt.imshow(w_ca3_ca1.detach().numpy(), aspect=\"auto\", cmap=\"RdYlGn\",\n",
    "           vmin=-1., vmax=1.)\n",
    "plt.title(\"$W_{CA3, CA1}$\")\n",
    "# plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6495e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "x_ca1 = w_ca3_ca1 @ x_ca3\n",
    "plt.subplot(211)\n",
    "plt.imshow(x_ca1.detach().numpy().T, aspect=\"auto\",\n",
    "              cmap=\"RdYlGn\", vmin=-1., vmax=1.)\n",
    "plt.title(\"$x_{CA1}$\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(IS.detach().numpy().T, aspect=\"auto\",\n",
    "              cmap=\"RdYlGn\", vmin=-1., vmax=1.)\n",
    "plt.title(\"$IS$\")\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ebf7c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- eigen-decomposition\n",
    "\n",
    "# EI -> CA1\n",
    "evals_ei_ca1, evect_ei_ca1  = np.linalg.eig(w_ei_ca1.detach().numpy())\n",
    "\n",
    "# CA3 -> CA1\n",
    "weica1 = w_ca3_ca1.detach().numpy() @ w_ei_ca3.detach().numpy()\n",
    "# evals_ca3_ca1, evect_ca3_ca1  = np.linalg.eig(w_ca3_ca1.detach().numpy())\n",
    "evals_ca3_ca1, evect_ca3_ca1  = np.linalg.eig(weica1)\n",
    "\n",
    "# plot\n",
    "plt.scatter(evals_ei_ca1.real, evals_ei_ca1.imag, label=\"$EI$->\", s=2)\n",
    "plt.scatter(evals_ca3_ca1.real, evals_ca3_ca1.imag, label=\"$CA3$->\", s=2)\n",
    "plt.legend()\n",
    "plt.xlabel(\"$Re$\")\n",
    "plt.ylabel(\"$Im$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7ca67",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" MTL declaration \"\"\"\n",
    "\n",
    "# get weights from the autoencoder\n",
    "W_ei_ca1, W_ca1_eo = autoencoder.get_weights()\n",
    "# W_ei_ca1 = torch.randn(dim_ca1, dim_ei)\n",
    "# W_ca1_eo = torch.randn(dim_eo, dim_ca1)\n",
    "\n",
    "# make model\n",
    "model = MTL(W_ei_ca1=W_ei_ca1,\n",
    "            W_ca1_eo=W_ca1_eo,\n",
    "            dim_ca3=dim_ca3,\n",
    "            lr=1.)\n",
    "model_rnd = MTL(W_ei_ca1=torch.randn(dim_ca1, dim_ei),\n",
    "            W_ca1_eo=torch.randn(dim_eo, dim_ca1),\n",
    "            dim_ca3=dim_ca3,\n",
    "            lr=1.)\n",
    "\n",
    "logger(f\"%MTL: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d76678",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" MODEL training \"\"\"\n",
    "\n",
    "epochs = 1\n",
    "for _ in range(epochs):\n",
    "    loss_mtl, model = utils.testing(data=training_sample_btsp,\n",
    "                                    model=model,\n",
    "                                    column=True)\n",
    "    loss_mtl_rnd, model_rnd = utils.testing(data=training_sample_btsp,\n",
    "                                            model=model_rnd,\n",
    "                                            column=True)\n",
    "    logger(f\"<<< MTL trained [{loss_mtl:.3f}] >>>\")\n",
    "    logger(f\"<<< MTL_random trained [{loss_mtl_rnd:.3f}] >>>\")\n",
    "\n",
    "# reconstruct data\n",
    "model.pause_lr()\n",
    "out_mtl, latent_mtl = utils.reconstruct_data(\n",
    "                                 data=training_sample_btsp,\n",
    "                                 num=num_btsp_samples,\n",
    "                                 model=model,\n",
    "                                 column=True,\n",
    "                                 plot=False)\n",
    "\n",
    "model_rnd.pause_lr()\n",
    "out_mtl_rnd, latent_mtl_rnd = utils.reconstruct_data(\n",
    "                                 data=training_sample_btsp,\n",
    "                                 num=num_btsp_samples,\n",
    "                                 model=model_rnd,\n",
    "                                 column=True,\n",
    "                                 plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6a1d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sample = torch.tensor(training_sample_btsp[0].reshape(-1, 1),\n",
    "                      requires_grad=False).float()\n",
    "print(sample.shape)\n",
    "out = model(sample)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef9170",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "-------------------------\n",
    "TESTING & VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931cf3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sample = 0\n",
    "\n",
    "data = training_sample_btsp[idx_sample].reshape(1, -1)\n",
    "\n",
    "if not isinstance(training_sample_btsp,\n",
    "                  DataLoader):\n",
    "    # Convert numpy array to torch tensor\n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    # Create a dataset and data loader\n",
    "    dataset = TensorDataset(data_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "else:\n",
    "    dataloader = data\n",
    "\n",
    "column = True\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "loss = 0.\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        inputs = batch[0] if not column else batch[0].reshape(-1, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # evaluate the output\n",
    "\n",
    "if idx_sample == 0:\n",
    "    outs1 = [model._ca3, model._ca1]\n",
    "\n",
    "    fig1, (ax11, ax12) = plt.subplots(2, 1, figsize=(20, 10))\n",
    "    ax11.imshow(outs1[0].reshape(1, -1), aspect=\"auto\", cmap=\"Greys_r\")\n",
    "    ax11.set_title(\"pattern 1 | CA3\")\n",
    "\n",
    "    ax12.imshow(outs1[1].reshape(1, -1), aspect=\"auto\", cmap=\"Greys_r\")\n",
    "    ax12.set_title(\"pattern 1 | CA1\")\n",
    "\n",
    "elif idx_sample == 1:\n",
    "\n",
    "    outs2 = [model._ca3, model._ca1]\n",
    "\n",
    "    fig1, (ax21, ax22) = plt.subplots(2, 1, figsize=(20, 10))\n",
    "    ax21.imshow(outs2[0].reshape(1, -1), aspect=\"auto\", cmap=\"Greys_r\")\n",
    "    ax21.set_title(\"pattern 2 | CA3\")\n",
    "\n",
    "    ax22.imshow(outs2[1].reshape(1, -1), aspect=\"auto\", cmap=\"Greys_r\")\n",
    "    ax22.set_title(\"pattern 2 | CA1\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234583f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "out2 = model._ca3\n",
    "out2\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.imshow(out2.reshape(1, -1), aspect=\"auto\", cmap=\"Greys_r\")\n",
    "ax2.set_title(\"pattern 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a87e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cafeae",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "np.dot(outs1[0], outs2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5fd0f",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" plotting \"\"\"\n",
    "\n",
    "# fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15, 5), sharex=True)\n",
    "\n",
    "# is_squash = False\n",
    "# utils.plot_squashed_data(\n",
    "#     # data=training_samples[:num_reconstructions].reshape(num_reconstructions, -1),\n",
    "#     data=training_sample_btsp,\n",
    "#                          ax=ax1,\n",
    "#                          title=\"Original\", squash=is_squash)\n",
    "# utils.plot_squashed_data(data=out_ae, ax=ax2,\n",
    "#                          title=\"Autoencoder\", squash=is_squash)\n",
    "# utils.plot_squashed_data(data=out_mtl, ax=ax3,\n",
    "#                          title=\"MTL\", squash=bool(1))\n",
    "# utils.plot_squashed_data(data=out_mtl_rnd, ax=ax4,\n",
    "#                          title=\"MTL (random)\", squash=is_squash)\n",
    "\n",
    "# fig.suptitle(\"Data reconstruction | all data vs first input\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 5), sharex=True)\n",
    "\n",
    "ax1.imshow(latent_ae[:5], cmap='gray_r', aspect='auto')\n",
    "ax1.set_ylabel(\"Autoencoder\")\n",
    "print(f\"\\n>>> latent_ae (last ECin input): {np.around(latent_ae[-1], 2)}\")\n",
    "\n",
    "ax2.imshow(latent_mtl[:5], cmap='gray_r', aspect='auto')\n",
    "ax2.set_ylabel(\"MTL\")\n",
    "print(f\"\\n>>> latent_mtl (last ECin input): {np.around(latent_mtl[-1], 2)}\")\n",
    "\n",
    "ax3.imshow(latent_mtl_rnd[:5], cmap='gray_r', aspect='auto')\n",
    "ax3.set_ylabel(\"MTL (random)\")\n",
    "print(f\"\\n>>> latent_mtl_rnd (last ECin input): {np.around(latent_mtl_rnd[-1], 2)}\")\n",
    "\n",
    "fig.suptitle(\"Latent space [CA1]\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
