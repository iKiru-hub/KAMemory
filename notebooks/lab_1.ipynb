{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef27949",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.expanduser('~/Research/lab/KAMemory/src'))\n",
    "import utils\n",
    "import torch\n",
    "from models import Autoencoder, MTL, logger\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10c547",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" settings \"\"\"\n",
    "\n",
    "# architecture sizes\n",
    "dim_ei = 50\n",
    "dim_ca3 = 50 \n",
    "dim_ca1 = 50\n",
    "dim_eo = dim_ei\n",
    "\n",
    "# data settings\n",
    "nb_samples = 300\n",
    "num_reconstructions = 1\n",
    "\n",
    "# distribution 1\n",
    "heads = 3\n",
    "variance = 0.05\n",
    "higher_heads = heads \n",
    "higher_variance = 0.075\n",
    "\n",
    "# make samples\n",
    "distrib_1 = utils.stimulus_generator(N=nb_samples, size=dim_ei,\n",
    "                             heads=heads, variance=variance,\n",
    "                             higher_heads=higher_heads,\n",
    "                             higher_variance=higher_variance,\n",
    "                             plot=False)\n",
    "test_distrib_1 = utils.stimulus_generator(N=nb_samples, size=dim_ei,\n",
    "                             heads=heads, variance=variance,\n",
    "                             higher_heads=higher_heads,\n",
    "                             higher_variance=higher_variance,\n",
    "                             plot=False)\n",
    "\n",
    "# distribution 2\n",
    "heads = 2\n",
    "variance = 0.05\n",
    "higher_heads = heads \n",
    "higher_variance = 0.075\n",
    "\n",
    "# make samples\n",
    "distrib_2 = utils.stimulus_generator(N=nb_samples, size=dim_ei,\n",
    "                             heads=heads, variance=variance,\n",
    "                             higher_heads=higher_heads,\n",
    "                             higher_variance=higher_variance,\n",
    "                             plot=False)\n",
    "test_distrib_2 = utils.stimulus_generator(N=nb_samples, size=dim_ei,\n",
    "                             heads=heads, variance=variance,\n",
    "                             higher_heads=higher_heads,\n",
    "                             higher_variance=higher_variance,\n",
    "                             plot=False)\n",
    "\n",
    "# make one data dataset\n",
    "if bool(1):\n",
    "    training_samples = np.concatenate((distrib_2, distrib_1), axis=0)\n",
    "    test_samples = np.concatenate((test_distrib_2, test_distrib_1), axis=0)\n",
    "\n",
    "    # shuffle\n",
    "    training_samples = training_samples[torch.randperm(training_samples.shape[0])]\n",
    "    test_samples = test_samples[torch.randperm(test_samples.shape[0])]\n",
    "    logger.debug(\"using both distributions\")\n",
    "\n",
    "else:\n",
    "    training_samples = distrib_2\n",
    "    test_samples = test_distrib_2\n",
    "\n",
    "# dataset for btsp\n",
    "num_btsp_samples = 2\n",
    "num_reconstructions = 1\n",
    "training_sample_btsp = training_samples[np.random.choice(range(training_samples.shape[0]),\n",
    "                                                         num_btsp_samples, replace=False)]\n",
    "\n",
    "\n",
    "logger(\"<<< Data generated >>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6565a18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" autoencoder training \"\"\"\n",
    "\n",
    "autoencoder = Autoencoder(input_dim=dim_ei,\n",
    "                          encoding_dim=dim_ca1)\n",
    "logger(f\"%Autoencoder: {autoencoder}\")\n",
    "\n",
    "# train autoencoder\n",
    "epochs = 200\n",
    "loss_ae, autoencoder = utils.train_autoencoder(\n",
    "                training_data=training_samples,\n",
    "                test_data=test_samples,\n",
    "                model=autoencoder,\n",
    "                epochs=int(epochs),\n",
    "                batch_size=5, learning_rate=1e-3)\n",
    "logger(f\"<<< Autoencoder trained [loss={loss_ae:.4f}] >>>\")\n",
    "\n",
    "# reconstruct data\n",
    "out_ae, latent_ae = utils.reconstruct_data(data=training_sample_btsp,\n",
    "                                num=num_btsp_samples,\n",
    "                                model=autoencoder,\n",
    "                                show=False, \n",
    "                                plot=False)\n",
    "\n",
    "\"\"\" mtl training \"\"\"\n",
    "\n",
    "# get weights from the autoencoder\n",
    "W_ei_ca1, W_ca1_eo = autoencoder.get_weights()\n",
    "# W_ei_ca1 = torch.randn(dim_ca1, dim_ei)\n",
    "# W_ca1_eo = torch.randn(dim_eo, dim_ca1)\n",
    "\n",
    "# make model\n",
    "model = MTL(W_ei_ca1=W_ei_ca1,\n",
    "            W_ca1_eo=W_ca1_eo,\n",
    "            dim_ca3=dim_ca3,\n",
    "            lr=1.)\n",
    "model_rnd = MTL(W_ei_ca1=torch.randn(dim_ca1, dim_ei),\n",
    "            W_ca1_eo=torch.randn(dim_eo, dim_ca1),\n",
    "            dim_ca3=dim_ca3,\n",
    "            lr=1.)\n",
    "\n",
    "logger(f\"%MTL: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db9227",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# train model\n",
    "epochs = 1\n",
    "for _ in range(epochs):\n",
    "    loss_mtl, model = utils.testing(data=training_sample_btsp,\n",
    "                                    model=model,\n",
    "                                    column=True)\n",
    "    loss_mtl_rnd, model_rnd = utils.testing(data=training_sample_btsp,\n",
    "                                            model=model_rnd,\n",
    "                                            column=True)\n",
    "    logger(f\"<<< MTL trained [{loss_mtl:.3f}] >>>\")\n",
    "    logger(f\"<<< MTL_random trained [{loss_mtl_rnd:.3f}] >>>\")\n",
    "\n",
    "# reconstruct data\n",
    "model.pause_lr()\n",
    "out_mtl, latent_mtl = utils.reconstruct_data(\n",
    "    data=training_sample_btsp,\n",
    "                                 num=num_btsp_samples,\n",
    "                                 model=model,\n",
    "                                 column=True,\n",
    "                                 plot=False)\n",
    "\n",
    "model_rnd.pause_lr()\n",
    "out_mtl_rnd, latent_mtl_rnd = utils.reconstruct_data(\n",
    "    data=training_sample_btsp,\n",
    "                                 num=num_btsp_samples,\n",
    "                                 model=model_rnd,\n",
    "                                 column=True,\n",
    "                                 plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5fd0f",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" plotting \"\"\"\n",
    "\n",
    "# fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15, 5), sharex=True)\n",
    "\n",
    "# is_squash = False\n",
    "# utils.plot_squashed_data(\n",
    "#     # data=training_samples[:num_reconstructions].reshape(num_reconstructions, -1),\n",
    "#     data=training_sample_btsp,\n",
    "#                          ax=ax1,\n",
    "#                          title=\"Original\", squash=is_squash)\n",
    "# utils.plot_squashed_data(data=out_ae, ax=ax2,\n",
    "#                          title=\"Autoencoder\", squash=is_squash)\n",
    "# utils.plot_squashed_data(data=out_mtl, ax=ax3,\n",
    "#                          title=\"MTL\", squash=bool(1))\n",
    "# utils.plot_squashed_data(data=out_mtl_rnd, ax=ax4,\n",
    "#                          title=\"MTL (random)\", squash=is_squash)\n",
    "\n",
    "# fig.suptitle(\"Data reconstruction | all data vs first input\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 5), sharex=True)\n",
    "\n",
    "ax1.imshow(latent_ae[:5], cmap='gray_r', aspect='auto')\n",
    "ax1.set_ylabel(\"Autoencoder\")\n",
    "print(f\"\\n>>> latent_ae (last ECin input): {np.around(latent_ae[-1], 2)}\")\n",
    "\n",
    "ax2.imshow(latent_mtl[:5], cmap='gray_r', aspect='auto')\n",
    "ax2.set_ylabel(\"MTL\")\n",
    "print(f\"\\n>>> latent_mtl (last ECin input): {np.around(latent_mtl[-1], 2)}\")\n",
    "\n",
    "ax3.imshow(latent_mtl_rnd[:5], cmap='gray_r', aspect='auto')\n",
    "ax3.set_ylabel(\"MTL (random)\")\n",
    "print(f\"\\n>>> latent_mtl_rnd (last ECin input): {np.around(latent_mtl_rnd[-1], 2)}\")\n",
    "\n",
    "fig.suptitle(\"Latent space [CA1]\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
